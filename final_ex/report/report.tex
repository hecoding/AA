\documentclass[titlepage]{article}
\usepackage{multicol}
\setlength{\columnsep}{0.6cm}
\usepackage[margin=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=black,       % color of internal links (change box color with linkbordercolor)
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=blue           % color of external links
}
\usepackage{graphicx}
\graphicspath{ {images/} }

\usepackage{color}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{amber}{rgb}{1.0, 0.49, 0.0}

\usepackage{listings}
\lstdefinestyle{code}{
	language=Octave,
	frame=single,
	breakatwhitespace=true,
	breaklines=true,
	basicstyle=\small\ttfamily,
	tabsize=4,
	numbers=left,
	numberstyle=\tiny,
	stepnumber=2,
	columns=fullflexible,
	backgroundcolor=\color{white},
	commentstyle=\color{mygreen},  % comment style
	keywordstyle=\color{blue},     % keyword style
	stringstyle=\color{amber},     % string literal style
	showstringspaces=false,
}
\lstdefinestyle{snippet}{
	language=Octave,
	breakatwhitespace=true,
	breaklines=true,
	basicstyle=\small\ttfamily,
}
\lstdefinestyle{output}{
	frame=single,
	breakatwhitespace=true,
	breaklines=true,
	basicstyle=\small\ttfamily,
}

\newenvironment{Figure} %para que se puedan meter imágenes en la columnas
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}


\title{Aprendizaje Automático \\ Proyecto de la asignatura}
\author{Héctor Laria Mantecón}
\date{14 de febrero de 2016}

\begin{document}

\maketitle

\begin{multicols*}{2}
[
\section*{\centering Introducción}
En este proyecto vamos a desarrollar varios sistemas de aprendizaje automático entrenados sobre el conjunto de datos escogido\footnote{\href{https://archive.ics.uci.edu/ml/datasets/CMU+Face+Images}{CMU Face Images Data Set}}. El objetivo de esto es comparar las distintas técnicas aprendidas en clase, dando mediciones y representaciones que nos permiten comparar dichas técnicas y sacar conclusiones sobre ellas.
]

\section{Explicación del dataset}
Los datos consisten en $640$ imágenes en blanco y negro de caras, tomadas con distinta \textbf{pose} (\textit{frente, izquierda, derecha, para arriba}), \textbf{expresión} (\textit{neutral, contento, triste, enfadado}) y \textbf{accesorios} (\textit{con gafas de sol, sin gafas de sol}). Las fotos vienen en tamaño normal, mediano  y pequeño.
\begin{Figure}
\centering
\includegraphics[width=0.49\linewidth]{{{ex1}}}
\includegraphics[width=0.49\linewidth]{{{ex2}}}
\includegraphics[width=0.49\linewidth]{{{ex3}}}
\includegraphics[width=0.49\linewidth]{{{ex4}}}
\end{Figure}

Cada imagen tiene una combinación de estos atributos, y hay 32 imágenes por cada persona. Sin embargo $16$ de las $640$ imágenes tuvieron errores al ser tomadas y no se han incluido en el conjunto.

\section{División de los datos}
Antes de nada vamos a dividir nuestros ejemplos de entrenamiento en 3 subconjuntos, \textit{entrenamiento}, \textit{validación} y \textit{test}. Cada uno tendrá un $60$, $20$ y $20\%$ de los datos de entrenamiento respectivamente.

Además es buena idea que estos estén \textbf{estratificados} y así cada uno tenga un poco de todo. Para ello en vez de juntar todo y dividir, vamos a hacer la división en cada clase y asignar cada subdivisión a un subconjunto de entrenamiento de los tres dichos.

Es una buena técnica separar de esta forma los datos para que los modelos entrenados no estén condicionados por haber visto unos ciertos ejemplos con anterioridad y así la predicción final no resulte afectada.
De esta manera, entrenaremos con el conjunto de \textit{entrenamiento} las variaciones de modelos que necesitemos considerar, probaremos estos modelos con el conjunto de \textit{validación}, y la predicción definitiva la haremos con el conjunto de \textit{test}.

\subsection{Tratamiento de los datos} \label{sec:tratamiento}
Sabemos que cada ejemplo es una imagen en escala de grises, es decir, un vector de números entre $0$ y $127$. Lo que haremos será dividirlo entre $127$ para reducir el rango entre $0$ y $1$. Después haremos \textit{normalización} para conseguir media $0$. No hace falta \textit{escalado} de atributos porque no hay grandes rangos entre ellos.

\section{Regresión logística}
Empezamos el entrenamiento con el método de regresión logística. No nos vamos a preocupar del grado del polinomio porque es suficientemente grande (exactamente una incógnita por píxel) y vamos a tratar el sobreajuste con el parámetro $\lambda$ de regularización. Así si por algún casual la regresión estuviera sesgada notaríamos el error en la curva de aprendizaje de $\lambda$.

Al comprobar primeramente su comportamiento sin hacer ningún ajuste podemos ver en la figura \ref{graf:log:ejemplos1} que no tiene el aspecto similar a las gráficas de este tipo, así que con la curva de aprendizaje de $\lambda$ vamos a tratar de arreglar esto.
\begin{Figure}
\includegraphics[width=\linewidth]{{{log_m_l=0}}}
\captionof{figure}{Error cuando los ejemplos aumentan}
\label{graf:log:ejemplos1}
\end{Figure}

La curva que vemos en la figura \ref{graf:log:lambdas} es peculiar y nos hace pensar que no es el mejor método para usar con estos datos, porque a partir de cierto punto siempre hay el mismo error y no se sesga nunca. Aún así vamos a coger la primera lambda con menor error para probarlo.
\begin{Figure}
\includegraphics[width=\linewidth]{{{log_lambdas}}}
\captionof{figure}{Error con respecto a la sucesión de lambdas}
\label{graf:log:lambdas}
\end{Figure}

La curva en la figura \ref{graf:log:ejemplos2} que resulta del entrenamiento con el parámetro escogido ($\lambda = 10000000$) sigue siendo igual de rara y podemos interpretar que está sesgada.
\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{log_m_l=10000000}}}
\captionof{figure}{Error vs nº de ejemplos. Nada común}
\label{graf:log:ejemplos2}
\end{Figure}

Al final, a sugerencia del profesor, escogeremos el parámetro de regularización que tenga más éxito en clasificar el conjunto de validación como prueba final (figura \ref{graf:log:porcentaje}).
\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{log_percentages_l=100}}}
\captionof{figure}{Ejemplos clasificados correctamente conforme avanza $\lambda$}
\label{graf:log:porcentaje}
\end{Figure}

Los resultados finales para cada clase y en general en el conjunto de \textit{test} son:

\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 \textbf{Clase} & \textbf{Aciertos} (\%) \\
 \hline \hline
 izquierda & 93.75 \\
 derecha & 74.19 \\
 frente & 54.84 \\
 arriba & 70.97 \\
 \hline
 \textbf{TOTAL} & \textbf{73.6} \\
 \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 Precisión & 0.736 \\
 Recall & 0.736 \\
 Average & 0.736 \\
 $F_1$ score & 0.736 \\
 \hline
\end{tabular}
\end{center}

\section{Redes neuronales}
Las \textit{redes neuronales} se basan en la regresión logística, pero al tener varias unidades de regresión y varias capas permite expresar modelos más complejos.
En las \textit{redes neuronales} una vez elegida una arquitectura no demasiado desproporcionada para la tarea que tenemos que hacer, sólo nos tenemos que preocupar del parámetro $\lambda$ de regularización para ajustar nuestra red y controlar el sesgo o la varianza.

\subsection{Entrenamiento}
Vamos a usar la arquitectura usada en la práctica de clasificación de números escritos ya que el trabajo que va a hacer es muy similar.

Para entrenar la red neuronal nos ayudamos de las \textit{curvas de aprendizaje} para ver cómo se comporta el modelo sin regularización. En la figura \ref{graf:nn:ejemplos1} vemos que el error no es muy alto (tendríamos \textit{sesgo}) pero sí hay un gran espacio entre las líneas de los ejemplos de entrenamiento y validación, lo que indica algo de \textit{varianza}. Con la gráfica de la figura \ref{graf:nn:lambdas} encontramos la mejor regularización, y ya podemos comprobar en la figura \ref{graf:nn:ejemplos2} que la diferencia de error se reduce.

Con el modelo convenientemente ajustado obtenemos los siguientes resultados:

\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 \textbf{Clase} & \textbf{Aciertos} (\%) \\
 \hline \hline
 izquierda & 93.75 \\
 derecha & 74.19 \\
 frente & 74.19 \\
 arriba & 74.19 \\
 \hline
 \textbf{TOTAL} & \textbf{79.2} \\
 \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 Precisión & 0.728 \\
 Recall & 0.728 \\
 Average & 0.728 \\
 $F_1$ score & 0.728 \\
 \hline
\end{tabular}
\end{center}

\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{nn_m_l=0}}}
\captionof{figure}{Error cuando los ejemplos aumentan}
\label{graf:nn:ejemplos1}
\end{Figure}

\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{nn_lambdas}}}
\captionof{figure}{Error con respecto a la sucesión de lambdas}
\label{graf:nn:lambdas}
\end{Figure}

\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{nn_m_l=0.3}}}
\captionof{figure}{Error vs nº ejemplos con regularización}
\label{graf:nn:ejemplos2}
\end{Figure}

\subsection{Reflexión}
Vemos que se mejoran los resultados obtenidos con regresión logística, pero detenidamente se nota que la mejora sólo procede de ciertas clases, sobre todo las que peor clasificaban antes. Al tener el mismo comportamiento pero mejor resultado podemos discernir que se trata del mismo método, pero en cierta manera más complejo.

Probamos las \textit{redes neuronales} como técnica de clasificación porque, al contrario que las \textit{SVM}s, pueden tener el número que queramos de salidas y se entrena una sola vez. Esto muchas veces puede ser determinante a por el contrario tener que entrenar por separado cada clase de un clasificador n-ario.
Pero por otra parte no se está aprovechando todo el potencial de las redes porque para los datos elegidos cada clase podría estar aislada, sin embargo al entrenarse a la vez las salidas siempre están interrelacionadas, lo que no nos hace falta.

\section{Support Vector Machines}

\subsection{Entrenamiento}
Para entrenar las \textit{SVM}s seguiremos el método utilizado en las prácticas; entrenar con diferentes valores para los parámetros y escoger el que más aciertos tenga (en el conjunto de validación). También haremos uso de diferentes \textit{kernels}.

\subsubsection{Kernel lineal}
Probando diferentes parámetros $C$ de regularización (figura \ref{graf:svm:lin}), escogemos $10$ como el que da mejor resultado.

\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 \textbf{Clase} & \textbf{Aciertos} (\%) \\
 \hline \hline
 izquierda & 91.2 \\
 derecha & 83.2 \\
 frente & 68 \\
 arriba & 68 \\
 \hline
 \textbf{TOTAL} & \textbf{77.6} \\
 \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 Precisión & 0.957 \\
 Recall & 0.686 \\
 Average & 0.822 \\
 $F_1$ score & 0.8 \\
 \hline
\end{tabular}
\end{center}

\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{svm_lin}}}
\captionof{figure}{Cuanto más cálido es el color del punto más tiempo ha tardado en entrenar}
\label{graf:svm:lin}
\end{Figure}

\subsubsection{Kernel gaussiano}
\hyperref[sec:tratamiento]{Como ya hemos dicho antes}, para nuestros datos no ha sido necesario pero es \textbf{muy} aconsejable \textit{escalar atributos} antes de usar el \textit{kernel gaussiano} ya que atributos muy grandes se pueden superponer encima de los pequeños.

En esta ocasión, a parte del parámetro $C$ también tenemos que ajustar el parámetro $\sigma$ propio del kernel, así que hacemos combinaciones de ambos y escogemos el que mejor se comporte. En la figura \ref{graf:svm:gauss:normal} hemos podido representarlo en 3D. Las figuras \ref{graf:svm:gauss:r} y \ref{graf:svm:gauss:l} son vistas a derecha e izquierda para mejor apreciación, y en la figura \ref{graf:svm:gauss:up} se muestra mejor el tiempo que se ha tardado en entrenar el modelo en concreto.

\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{svm_gauss_normal}}}
\captionof{figure}{Cuanto más grande es el punto más tiempo tarda en entrenar. El color va en relación al tamaño, cuanto maś grande más cálido}
\label{graf:svm:gauss:normal}
\end{Figure}

\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{svm_gauss_r}}}
\captionof{figure}{}
\label{graf:svm:gauss:r}
\end{Figure}

\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{svm_gauss_l}}}
\captionof{figure}{}
\label{graf:svm:gauss:l}
\end{Figure}

\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{svm_gauss_up}}}
\captionof{figure}{Tiempo que tarda en entrenar para cada $C$ y $\sigma$}
\label{graf:svm:gauss:up}
\end{Figure}

Los mejores parámetros son $C = 0.01$ y $\sigma = 3$ y los resultados obtenidos con ellos son los siguientes:

\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 \textbf{Clase} & \textbf{Aciertos} (\%) \\
 \hline \hline
 izquierda & 92 \\
 derecha & 80.8 \\
 frente & 82.4 \\
 arriba & 82.4 \\
 \hline
 \textbf{TOTAL} & \textbf{84.4} \\
 \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{ |c|c| }
 \hline
 Precisión & 0.844 \\
 Recall & 0.844 \\
 Average & 0.844 \\
 $F_1$ score & 0.844 \\
 \hline
\end{tabular}
\end{center}

\subsection{Reflexión}
La \textit{SVM} con \textit{kernel lineal} no lo hace mejor que las \textit{redes neuronales}, pero vamos a hablar poco de él porque se ha demostrado\footnote{\href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.141.880&rep=rep1&type=pdf}{Asymptotic Behaviors of Support Vector Machines with Gaussian Kernel}} que el \textit{kernel lineal} es una versión degenerada del \textit{kernel gaussiano}, por tanto nunca va a comportarse mejor que éste \footnote{Aún así es útil porque si clasifica suficientemente bien para nuestros datos, es mucho más rápido de entrenar. Su mejor aplicación es cuando hay muchos atributos, ya que se acerca más a los resultados del \textit{kernel gaussiano}.}. Con \textit{kernel gaussiano} sí vemos que actúa mucho mejor para casi todas las clases, ya que solo mira si la clase dada aparece.

Como ya hemos dicho antes, en lo que a tiempo se refiere es menos ventajoso entrenar un clasificador n-ario de \textit{SVM} porque hay que entrenar cada clase por separado, pero cuando las salidas no tienen relación entre ellas, hacerlo de este modo tiene mucho más sentido.
Para nuestros datos si lo que queremos es clasificar direcciones de la cabeza, $4$ \textit{SVM} sería la manera de hacerlo. Esto es porque la foto tiene la información de una dirección (una clase) y nunca tendrá más de una, por tanto tiene mucho más sentido que utilizar \textit{redes neuronales} (donde las salidas sí están interrelacionadas).

\section{Conclusión}
Estos son los resultados finales obtenidos con cada técnica de clasificación, junto con el tiempo empleado en entrenar el clasificador que más ha acertado, junto con el tiempo de entrenar todos los clasificadores probados:

\begin{center}
\begin{tabular}{ |l|c|c|c| }
 \hline
 \multicolumn{1}{|c|}{\textbf{Técnica}} & \textbf{\%} & \textbf{Tiempo} (s) & \textbf{T. total} (s) \\
 \hline
 \textbf{Reg. Log.} & 73.6 & 7.61 & 74.38 \\
 \textbf{NN} & 79.2 & 17.61 & 156.19 \\
 \textbf{SVM} (lineal) & 77.6 & 22.57 & 176.89 \\
 \textbf{SVM} (gauss) & 84.4 & 5.33 & 626.55 \\
 \hline
\end{tabular}
\end{center}

Estas son las medidas de precisión y recall:

\begin{center}
\begin{tabular}{ |l|c|c||c|c| }
 \hline
 \multicolumn{1}{|c|}{\textbf{Técnica}} & \textbf{Precis.} & \textbf{Recall} & \textbf{Media} & \textbf{$F_1$} \\
 \hline
 \textbf{Reg. Log.} & 0.736 & 0.736 & 0.736 & 0.736 \\
 \textbf{NN} & 0.957 & 0.686 & 0.822 & 0.8 \\
 \textbf{SVM} (lineal) & 0.736 & 0.736 & 0.736 & 0.736 \\
 \textbf{SVM} (gauss) & 0.844 & 0.844 & 0.844 & 0.844 \\
 \hline
\end{tabular}
\end{center}

El mejor clasificador ha sido el de \textit{SVM} con \textit{kernel gaussiano}. Al contrario de lo que ponía en los apuntes (si hay muchos atributos ($3840$) y pocos ejemplos de entrenamiento ($624$), usar regresión logística o \textit{SVM} con \textit{kernel lineal}).

Creo que este resultado ha sido debido a la naturaleza de los datos. Resumiendo lo dicho antes, es mucho mejor utilizar \textit{SVM}s que \textit{redes neuronales} ya que no nos hace falta la funcionalidad innata de las redes de tener salidas interrelaccionadas. Como en este caso un ejemplo sólo puede tener información de una clase lo mejor es tener clasificadores aislados para cada salida que se fijen en su propia clase y descarten las demás.

Los resultados hubieran sido diferentes si hubiéramos tenido el mismo número de atributos y de ejemplos de entrenamiento, pero por ejemplo hubieramos querido modelar algo biológico como el balance de hormonas de una persona dados unos factores. En ese caso la interrelación es crucial. La regresión logística y las \textit{redes neuronales} hubieran sido mejores. ¿Cuál de los dos? Depende mucho pero hubiera tenido que ver con el conjunto de datos concreto.

\pagebreak
\section{Otras técnicas}
\subsection{Reducción de dimensiones}
Vamos a comprimir los datos para tratar de acelerar el entrenamiento. Utilizamos el algoritmo \textit{PCA} para conseguir la matriz de \textit{autovectores} necesaria para comprimir los datos en diferentes dimensiones.

Con esta matriz podemos visualizar los datos en una gráfica reduciéndolos a $2$ dimensiones como en la figura \ref{graf:dim_red:2d}.
\begin{Figure}
\centering
\includegraphics[width=\linewidth]{{{dim_2d}}}
\captionof{figure}{Visualización 2D}
\label{graf:dim_red:2d}
\end{Figure}
Para comprimir los datos sin afectar a la calidad lo más posible, buscamos una \textbf{varianza del $99\%$}. Eso lo conseguimos con una reducción de $3840$ a simplemente $165$. Cuando volvemos a descomprimir y visualizar los datos vemos que aunque la calidad ha bajado se distingue perfectamente qué tipo de foto es.
\begin{Figure}
%\centering
\includegraphics[width=0.48\linewidth]{{{dim_original}}}
\includegraphics[width=0.48\linewidth]{{{dim_red}}}
\captionof{figure}{Foto original y foto comprimida y descomprimida}
\label{graf:dim_red:}
\end{Figure}

A la hora del entrenamiento obtenemos los siguientes resultados:
\begin{center}
\begin{tabular}{ |l|c|c|c| }
 \hline
 \multicolumn{1}{|c|}{\textbf{Técnica}} & \textbf{\%} & \textbf{Tiempo} (s) & \textbf{T. total} (s) \\
 \hline
 \textbf{Reg. Log.} & 59.2 & 0.14 & 3.87 \\
 \textbf{NN} & 72 & 10.28 & 42.63 \\
 \textbf{SVM} (lineal) & 84.8 & 22.77 & 220.88 \\
 \textbf{SVM} (gauss) & 68.8 & 5.78 & 658.34 \\
 \hline
\end{tabular}
\end{center}

Donde el mejor clasificador sigue siendo la \textit{SVM}, pero en este caso el \textit{kernel lineal} \footnote{esto es porque no hace falta mapear los datos a otras dimensiones como hace el \textit{kernel gaussiano}}. También vemos que para la regresión logística y las redes funciona mejor, pero sin embargo el tiempo se incrementa o queda igual en la \textit{SVM} independientemente del kernel utilizado.

\subsection{Engañando a la red neuronal}
Ya hemos visto cómo funcionan las redes con los ejemplos que le hemos dado, o con ejemplos que podrían darle, pero ¿cómo reacciona cuando le damos una entrada que no tiene sentido?

\begin{Figure}
%\centering
\includegraphics[width=0.48\linewidth]{{{trick_black}}}
\includegraphics[width=0.48\linewidth]{{{trick_white}}}
\captionof{figure}{Una imagen negra, una imagen blanca}
\label{graf:dim_red:}
\end{Figure}

Vamos a probar con una imagen completamente negra.
\begin{lstlisting}[style=output]
>> black = zeros(1, size(Xtrain, 1));
>> whatIs(black, 1)
1.66774% left
0.726512% right
0.198064% straight
0.147015% up

it's left with 1.66774%
\end{lstlisting}

No lo tiene muy claro pero piensa que es de la clase de la izquierda. Vamos a ver qué pasa con lo contrario, una imagen completamente blanca.
\begin{lstlisting}[style=output]
>> white = ones(1, size(Xtrain, 1));
>> whatIs(white, 1)
0.000121845% left
21.3542% right
0.0197489% straight
99.3715% up

it's up with 99.3715%
\end{lstlisting}

Es mucho mejor, está completamente segura que es una persona mirando hacia arriba.
Esto nos hace pensar en cómo busca la red neuronal en la imagen para decir que es una cosa u otra.

Para hacer eso necesitamos calcular el \textbf{gradiente} de la red neuronal, que se consigue con la salida del algoritmo de progagación hacia atrás que ya implementamos en una práctica (aunque ligeramente modificado).

\begin{lstlisting}[style=output]
% attributes = {'left', 'right', 'straight', 'up'};

>> delta = backPropagation(black, [0 1 0 0]);
>> imshow(reshape(delta,60,64) * 300)

>> imshow(reshape(backPropagation(black, [1 0 0 0]),60,64))

>> imshow(reshape(backPropagation(black, [0 1 0 0]),60,64))
\end{lstlisting}

\begin{Figure}
%\centering
\includegraphics[width=0.48\linewidth]{{{trick_delta_r}}}
\includegraphics[width=0.48\linewidth]{{{trick_delta_l}}}
\captionof{figure}{Lo que busca de una persona mirando a la derecha y a la izquierda}
\label{graf:dim_red:}
\end{Figure}

Como podemos ver, el gradiente de la persona mirando a la izquierda se parece mucho más a una persona que la del gradiente a la derecha. Esto significa que tiene más claro qué es una persona mirando a la izquierda, y se nota mirando en los resultados del entrenamiento anterior, donde la clase de la izquierda alcanzó un $93.75\%$ de aciertos.

Ahora bien, teniendo lo que busca la red para predecir si es una clase u otra, ¿podemos usarlo en su contra?

\begin{lstlisting}[style=output]
>> whatIs(Xtest(:,1)',1)
0.0104844% left
86.5416% right
2.72964e-05% straight
75.9797% up

it's right with 86.5416%

>> delta = backPropagation(black, [0 1 0 0]);

>> whatIs(Xtest(:,1)' - delta * 9.8 ,1)
0.00288016% left
84.1261% right
0.000111166% straight
84.1638% up

it's up with 84.1638%
\end{lstlisting}

\begin{Figure}
%\centering
\includegraphics[width=0.48\linewidth]{{{trick_test}}}
\includegraphics[width=0.48\linewidth]{{{trick_test_tricked}}}
\captionof{figure}{A la izquierda la imagen original, a la derecha la imagen modificada}
\label{graf:dim_red:}
\end{Figure}

Como vemos la diferencia es imperceptible pero la predicción de la red es muy diferente.
La mejor forma que se conoce de prevenir esto es entrenar la red con muchos ejemplos de entrenamiento, variados a ser posible\footnote{\href{https://codewords.recurse.com/issues/five/why-do-neural-networks-think-a-panda-is-a-vulture}{Este apartado fue inspirado gracias a este artículo}}.

\subsection{Otros}
Diversas cosas como clustering, la prueba de otros métodos no vistos con \texttt{Weka}, o sliding de las gafas de sol se han quedado en el tintero por falta de tiempo.

\end{multicols*}

\section{Código utilizado}
El código para hacer los entrenamientos ha sido el de las prácticas anteriormente implementadas. He escrito mi propio wrapper (o API) para usarlo. El código (que también se puede encontrar en \href{https://github.com/hecoding/machine-learning-class/tree/master/final_ex}{GitHub} es el siguiente:

\lstlistoflistings

\lstinputlisting[style=code,caption=main.m]{../main.m}
\lstinputlisting[style=code,caption=makeSets.m]{../makeSets.m}
\lstinputlisting[style=code,caption=splitDataSet.m]{../splitDataSet.m}
\lstinputlisting[style=code,caption=importImages.m]{../importImages.m}
\lstinputlisting[style=code,caption=runClustering.m]{../runClustering.m}
\lstinputlisting[style=code,caption=runDimReduction.m]{../runDimReduction.m}
\lstinputlisting[style=code,caption=runLogReg.m]{../runLogReg.m}
\lstinputlisting[style=code,caption=runNNTraining.m]{../runNNTraining.m}
\lstinputlisting[style=code,caption=runSVMTraining.m]{../runSVMTraining.m}
\lstinputlisting[style=code,caption=compressData.m]{../compressData.m}
\lstinputlisting[style=code,caption=decompressData.m]{../decompressData.m}
\lstinputlisting[style=code,caption=logistic\_regression/computePercentageOneVsAll.m]{../logistic_regression/computePercentageOneVsAll.m}
\lstinputlisting[style=code,caption=logistic\_regression/displayData.m]{../logistic_regression/displayData.m}
\lstinputlisting[style=code,caption=logistic\_regression/featureNormalize.m]{../logistic_regression/featureNormalize.m}
\lstinputlisting[style=code,caption=logistic\_regression/fmincg.m]{../logistic_regression/fmincg.m}
\lstinputlisting[style=code,caption=logistic\_regression/lrCostFunction.m]{../logistic_regression/lrCostFunction.m}
\lstinputlisting[style=code,caption=logistic\_regression/newCost.m]{../logistic_regression/newCost.m}
\lstinputlisting[style=code,caption=logistic\_regression/oneVsAll.m]{../logistic_regression/oneVsAll.m}
\lstinputlisting[style=code,caption=logistic\_regression/plotError.m]{../logistic_regression/plotError.m}
\lstinputlisting[style=code,caption=logistic\_regression/plotLambda.m]{../logistic_regression/plotLambda.m}
\lstinputlisting[style=code,caption=logistic\_regression/plotPercentage.m]{../logistic_regression/plotPercentage.m}
\lstinputlisting[style=code,caption=logistic\_regression/predict.m]{../logistic_regression/predict.m}
\lstinputlisting[style=code,caption=logistic\_regression/sig.m]{../logistic_regression/sig.m}
\lstinputlisting[style=code,caption=neural\_network/backPropagation.m]{../neural_network/backPropagation.m}
\lstinputlisting[style=code,caption=neural\_network/costNN.m]{../neural_network/costNN.m}
\lstinputlisting[style=code,caption=neural\_network/featureNormalize.m]{../neural_network/featureNormalize.m}
\lstinputlisting[style=code,caption=neural\_network/fmincg.m]{../neural_network/fmincg.m}
\lstinputlisting[style=code,caption=neural\_network/forwardProp.m]{../neural_network/forwardProp.m}
\lstinputlisting[style=code,caption=neural\_network/forwardProp\_pure.m]{../neural_network/forwardProp_pure.m}
\lstinputlisting[style=code,caption=neural\_network/plotError.m]{../neural_network/plotError.m}
\lstinputlisting[style=code,caption=neural\_network/plotLambda.m]{../neural_network/plotLambda.m}
\lstinputlisting[style=code,caption=neural\_network/plotPercentage.m]{../neural_network/plotPercentage.m}
\lstinputlisting[style=code,caption=neural\_network/randWeights.m]{../neural_network/randWeights.m}
\lstinputlisting[style=code,caption=neural\_network/runNN.m]{../neural_network/runNN.m}
\lstinputlisting[style=code,caption=neural\_network/sigDeriv.m]{../neural_network/sigDeriv.m}
\lstinputlisting[style=code,caption=neural\_network/sig.m]{../neural_network/sig.m}
\lstinputlisting[style=code,caption=neural\_network/whatIs.m]{../neural_network/whatIs.m}
\lstinputlisting[style=code,caption=SVM/computePercentageOneVsAll.m]{../SVM/computePercentageOneVsAll.m}
\lstinputlisting[style=code,caption=SVM/featureNormalize.m]{../SVM/featureNormalize.m}
\lstinputlisting[style=code,caption=SVM/gaussianKernel.m]{../SVM/gaussianKernel.m}
\lstinputlisting[style=code,caption=SVM/linearKernel.m]{../SVM/linearKernel.m}
\lstinputlisting[style=code,caption=SVM/predictOneVsAll.m]{../SVM/predictOneVsAll.m}
\lstinputlisting[style=code,caption=SVM/splitDataSet.m]{../SVM/splitDataSet.m}
\lstinputlisting[style=code,caption=SVM/svmPredict.m]{../SVM/svmPredict.m}
\lstinputlisting[style=code,caption=SVM/svmTrain.m]{../SVM/svmTrain.m}
\lstinputlisting[style=code,caption=SVM/trainOneVsAll.m]{../SVM/trainOneVsAll.m}
\lstinputlisting[style=code,caption=dimensionality\_reduction/chooseK.m]{../dimensionality_reduction/chooseK.m}
\lstinputlisting[style=code,caption=dimensionality\_reduction/pca.m]{../dimensionality_reduction/pca.m}
\lstinputlisting[style=code,caption=dimensionality\_reduction/plotData.m]{../dimensionality_reduction/plotData.m}
\lstinputlisting[style=code,caption=dimensionality\_reduction/preprocessData.m]{../dimensionality_reduction/preprocessData.m}
\lstinputlisting[style=code,caption=clustering/computeCentroids.m]{../clustering/computeCentroids.m}
\lstinputlisting[style=code,caption=clustering/displayData.m]{../clustering/displayData.m}
\lstinputlisting[style=code,caption=clustering/drawLine.m]{../clustering/drawLine.m}
\lstinputlisting[style=code,caption=clustering/findClosestCentroids.m]{../clustering/findClosestCentroids.m}
\lstinputlisting[style=code,caption=clustering/plotDataPoints.m]{../clustering/plotDataPoints.m}
\lstinputlisting[style=code,caption=clustering/plotProgresskMeans.m]{../clustering/plotProgresskMeans.m}
\lstinputlisting[style=code,caption=clustering/runkMeans.m]{../clustering/runkMeans.m}

\end{document}